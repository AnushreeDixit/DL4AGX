diff --git a/research/deeplab/common.py b/research/deeplab/common.py
index 83b73f07..fc11decc 100644
--- a/research/deeplab/common.py
+++ b/research/deeplab/common.py
@@ -26,10 +26,11 @@ flags = tf.app.flags
 
 # Flags for input preprocessing.
 
-flags.DEFINE_integer('min_resize_value', None,
+# Set resize_values as 547x165 for kitti lane dataset.
+flags.DEFINE_integer('min_resize_value', 165,
                      'Desired size of the smaller image side.')
 
-flags.DEFINE_integer('max_resize_value', None,
+flags.DEFINE_integer('max_resize_value', 547,
                      'Maximum allowed size of the larger image side.')
 
 flags.DEFINE_integer('resize_factor', None,
@@ -50,7 +51,10 @@ flags.DEFINE_string('model_variant', 'mobilenet_v2', 'DeepLab model variant.')
 flags.DEFINE_multi_float('image_pyramid', None,
                          'Input scales for multi-scale feature extraction.')
 
-flags.DEFINE_boolean('add_image_level_feature', True,
+# Set add_image_level_feature is False as this feature includes tf.Resize_Bilinear
+# operation which is not supported by TensorRT.
+
+flags.DEFINE_boolean('add_image_level_feature', False,
                      'Add image level feature.')
 
 flags.DEFINE_multi_integer(
@@ -62,7 +66,8 @@ flags.DEFINE_multi_integer(
 flags.DEFINE_boolean('aspp_with_batch_norm', True,
                      'Use batch norm parameters for ASPP or not.')
 
-flags.DEFINE_boolean('aspp_with_separable_conv', True,
+# Set aspp_with_separable_conv False. TensorRT does not support separable_conv at this moment.
+flags.DEFINE_boolean('aspp_with_separable_conv', False,
                      'Use separable convolution for ASPP or not.')
 
 # Defaults to None. Set multi_grid = [1, 2, 4] when using provided
diff --git a/research/deeplab/core/feature_extractor.py b/research/deeplab/core/feature_extractor.py
index da89dfe9..01dc831b 100644
--- a/research/deeplab/core/feature_extractor.py
+++ b/research/deeplab/core/feature_extractor.py
@@ -78,6 +78,8 @@ networks_map = {
     'xception_41': xception.xception_41,
     'xception_65': xception.xception_65,
     'xception_71': xception.xception_71,
+    'resnet18': resnet_v1_beta.resnet18,
+    'resnet18_nvidia_deeplab': resnet_v1_beta.resnet18_nvidia_deeplab,
 }
 
 # A map from network name to network arg scope.
@@ -90,6 +92,8 @@ arg_scopes_map = {
     'xception_41': xception.xception_arg_scope,
     'xception_65': xception.xception_arg_scope,
     'xception_71': xception.xception_arg_scope,
+    'resnet18': resnet_utils.resnet_arg_scope,
+    'resnet18_nvidia_deeplab': resnet_utils.resnet_arg_scope,
 }
 
 # Names for end point features.
@@ -143,6 +147,8 @@ name_scope = {
     'xception_41': 'xception_41',
     'xception_65': 'xception_65',
     'xception_71': 'xception_71',
+    'resnet18': 'resnet18',
+    'resnet18_nvidia_deeplab': 'resnet18_nvidia_deeplab',
 }
 
 # Mean pixel value.
@@ -169,6 +175,8 @@ _PREPROCESS_FN = {
     'xception_41': _preprocess_zero_mean_unit_range,
     'xception_65': _preprocess_zero_mean_unit_range,
     'xception_71': _preprocess_zero_mean_unit_range,
+    'resnet18': _preprocess_subtract_imagenet_mean,
+    'resnet18_nvidia_deeplab': _preprocess_subtract_imagenet_mean,
 }
 
 
diff --git a/research/deeplab/core/resnet_v1_beta.py b/research/deeplab/core/resnet_v1_beta.py
index 91b72e43..3f9fb8ba 100644
--- a/research/deeplab/core/resnet_v1_beta.py
+++ b/research/deeplab/core/resnet_v1_beta.py
@@ -94,6 +94,107 @@ def bottleneck(inputs,
                                             output)
 
 
+@slim.add_arg_scope
+def bottleneck_resnet18(inputs,
+                        depth,
+                        depth_bottleneck,
+                        stride,
+                        unit_rate=1,
+                        rate=1,
+                        outputs_collections=None,
+                        scope=None):
+  """This is the bottleneck for ResNet-18 proposed in [1] only.
+
+  Bottleneck residual unit variant with BN after convolutions.
+
+  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for
+  its definition. Note that we use here the bottleneck variant which has an
+  extra bottleneck layer.
+
+  When putting together two consecutive ResNet blocks that use this unit, one
+  should use stride = 2 in the last unit of the first block.
+
+  Args:
+    inputs: A tensor of size [batch, height, width, channels].
+    depth: The depth of the ResNet unit output.
+    depth_bottleneck: The depth of the bottleneck layers.
+    stride: The ResNet unit's stride. Determines the amount of downsampling of
+      the units output compared to its input.
+    unit_rate: An integer, unit rate for atrous convolution.
+    rate: An integer, rate for atrous convolution.
+    outputs_collections: Collection to add the ResNet unit output.
+    scope: Optional variable_scope.
+
+  Returns:
+    The ResNet unit's output.
+  """
+  with tf.variable_scope(scope, 'bottleneck_resnet18', [inputs]) as sc:
+    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)
+    if depth == depth_in:
+      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')
+    else:
+      shortcut = slim.conv2d(
+          inputs,
+          depth,
+          [1, 1],
+          stride=stride,
+          activation_fn=None,
+          scope='shortcut')
+
+
+    residual = slim.conv2d(inputs, depth_bottleneck, 3, stride=1,
+                           scope='conv1')
+
+    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,
+                                        rate=rate*unit_rate, scope='conv2')
+
+    output = tf.nn.relu(shortcut + residual)
+
+    return slim.utils.collect_named_outputs(outputs_collections,
+                                            sc.name,
+                                            output)
+
+
+@slim.add_arg_scope
+def bottleneck_resnet18_nvidia(inputs,
+                               depth,
+                               depth_bottleneck,
+                               stride,
+                               unit_rate=1,
+                               rate=1,
+                               first_block=False,
+                               outputs_collections=None,
+                               scope=None):
+  """This is the bottleneck for a specific ResNet-18 version optimized
+     by Nvidia TensorRT.
+  """
+  with tf.variable_scope(scope, 'bottleneck_resnet18_nvidia', [inputs]) as sc:
+    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)
+    if depth == depth_in and first_block == False:
+      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')
+    else:
+      shortcut = slim.conv2d(
+          inputs,
+          depth,
+          [1, 1],
+          stride=stride,
+          activation_fn=None,
+          scope='shortcut')
+
+
+    residual = slim.conv2d(inputs, depth_bottleneck, 3, stride=1,
+                           scope='conv1')
+
+    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,
+                                        rate=rate*unit_rate, scope='conv2')
+
+    output = tf.nn.relu(shortcut + residual)
+
+    return slim.utils.collect_named_outputs(outputs_collections,
+                                            sc.name,
+                                            output)
+
+
 def root_block_fn_for_beta_variant(net):
   """Gets root_block_fn for beta variant.
 
@@ -175,7 +276,9 @@ def resnet_v1_beta(inputs,
   with tf.variable_scope(scope, 'resnet_v1', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     with slim.arg_scope([slim.conv2d, bottleneck,
-                         resnet_utils.stack_blocks_dense],
+                         resnet_utils.stack_blocks_dense,
+                         bottleneck_resnet18,
+                         bottleneck_resnet18_nvidia],
                         outputs_collections=end_points_collection):
       if is_training is not None:
         arg_scope = slim.arg_scope([slim.batch_norm], is_training=is_training)
@@ -231,6 +334,212 @@ def resnet_v1_beta_block(scope, base_depth, num_units, stride):
   }])
 
 
+def resnet18_block(scope, base_depth, num_units, stride):
+  """Helper function for creating a resnet-18 bottleneck block.
+
+  Args:
+    scope: The scope of the block.
+    base_depth: The depth of the bottleneck layer for each unit.
+    num_units: The number of units in the block.
+    stride: The stride of the block, implemented as a stride in the last unit.
+      All other units have stride=1.
+
+  Returns:
+    A resnet18 bottleneck block.
+  """
+  return resnet_utils.Block(scope, bottleneck_resnet18, [{
+      'depth': base_depth,
+      'depth_bottleneck': base_depth,
+      'stride': 1,
+      'unit_rate': 1
+  }] * (num_units - 1) + [{
+      'depth': base_depth,
+      'depth_bottleneck': base_depth,
+      'stride': stride,
+      'unit_rate': 1
+  }])
+
+
+def resnet18_nvidia_block(scope, base_depth, num_units, stride, first_block=False):
+  """Helper function for creating a resnet-18-nvidia bottleneck block.
+
+  Args:
+    scope: The scope of the block.
+    base_depth: The depth of the bottleneck layer for each unit.
+    num_units: The number of units in the block.
+    stride: The stride of the block, implemented as a stride in the last unit.
+      All other units have stride=1.
+
+  Returns:
+    A resnet18_nvidia bottleneck block.
+  """
+  # Expect num_units to be 1 or 2
+  if not(num_units in [1,2]):
+      raise ValueError('Expect num_units to be 1 or 2.')
+
+  if num_units == 2:
+      base_depth_front = base_depth[0]
+      base_depth_back = base_depth[1]
+  else:
+      base_depth_front = None
+      base_depth_back = base_depth[0]
+
+  return resnet_utils.Block(scope, bottleneck_resnet18_nvidia, [{
+      'depth': base_depth_front,
+      'depth_bottleneck': base_depth_front,
+      'stride': 1,
+      'unit_rate': 1,
+  }] * (num_units - 1) + [{
+      'depth': base_depth_back,
+      'depth_bottleneck': base_depth_back,
+      'stride': stride,
+      'unit_rate': 1,
+      'first_block': first_block
+  }])
+
+
+def resnet18(inputs,
+             num_classes=None,
+             is_training=None,
+             global_pool=False,
+             output_stride=None,
+             multi_grid=None,
+             reuse=None,
+             scope='resnet18'):
+  """ResNet-18.
+
+  Args:
+    inputs: A tensor of size [batch, height_in, width_in, channels].
+    num_classes: Number of predicted classes for classification tasks. If None
+      we return the features before the logit layer.
+    is_training: Enable/disable is_training for batch normalization.
+    global_pool: If True, we perform global average pooling before computing the
+      logits. Set to True for image classification, False for dense prediction.
+    output_stride: If None, then the output will be computed at the nominal
+      network stride. If output_stride is not None, it specifies the requested
+      ratio of input to output spatial resolution.
+    multi_grid: Employ a hierarchy of different atrous rates within network.
+    reuse: whether or not the network and its variables should be reused. To be
+      able to reuse 'scope' must be given.
+    scope: Optional variable_scope.
+
+  Returns:
+    net: A rank-4 tensor of size [batch, height_out, width_out, channels_out].
+      If global_pool is False, then height_out and width_out are reduced by a
+      factor of output_stride compared to the respective height_in and width_in,
+      else both height_out and width_out equal one. If num_classes is None, then
+      net is the output of the last ResNet block, potentially after global
+      average pooling. If num_classes is not None, net contains the pre-softmax
+      activations.
+    end_points: A dictionary from components of the network to the corresponding
+      activation.
+
+  Raises:
+    ValueError: if multi_grid is not None and does not have length = 3.
+  """
+  if multi_grid is None:
+    multi_grid = _DEFAULT_MULTI_GRID
+  else:
+    if len(multi_grid) != 3:
+      raise ValueError('Expect multi_grid to have length 3.')
+  blocks = [
+      resnet18_block(
+          'block1', base_depth=64, num_units=2, stride=2),
+      resnet18_block(
+          'block2', base_depth=128, num_units=2, stride=2),
+      resnet18_block(
+          'block3', base_depth=256, num_units=2, stride=2),
+      resnet_utils.Block('block4', bottleneck_resnet18, [
+          {'depth': 512,
+           'depth_bottleneck': 512,
+           'stride': 1,
+           'unit_rate': rate} for rate in multi_grid]),
+  ]
+  return resnet_v1_beta(
+      inputs,
+      blocks=blocks,
+      num_classes=num_classes,
+      is_training=is_training,
+      global_pool=global_pool,
+      output_stride=output_stride,
+      reuse=reuse,
+      scope=scope)
+
+def resnet18_nvidia_deeplab(inputs,
+                            num_classes=None,
+                            is_training=None,
+                            global_pool=False,
+                            output_stride=None,
+                            multi_grid=None,
+                            reuse=None,
+                            scope='resnet18_nvidia'):
+  """DeepLab model with ResNet-18 optimized by Nvidia TensorRT."""
+  if multi_grid is None:
+    multi_grid = _DEFAULT_MULTI_GRID
+  else:
+    if len(multi_grid) != 3:
+      raise ValueError('Expect multi_grid to have length 3.')
+  blocks = [
+      resnet18_nvidia_block(
+          'block1', base_depth=[64,64], num_units=1, stride=1, first_block=True),
+      resnet18_nvidia_block(
+          'block2', base_depth=[64,128], num_units=2, stride=2),
+      resnet18_nvidia_block(
+          'block3', base_depth=[128,256], num_units=2, stride=2),
+      resnet_utils.Block('block4', bottleneck_resnet18, [
+          {'depth': 512,
+           'depth_bottleneck': 512,
+           'stride': 1,
+           'unit_rate': rate} for rate in multi_grid]),
+  ]
+  return resnet_v1_beta(
+      inputs,
+      blocks=blocks,
+      num_classes=num_classes,
+      is_training=is_training,
+      global_pool=global_pool,
+      output_stride=output_stride,
+      reuse=reuse,
+      scope=scope)
+
+
+def resnet18_nvidia(inputs,
+                    num_classes=None,
+                    is_training=None,
+                    global_pool=False,
+                    output_stride=None,
+                    multi_grid=None,
+                    reuse=None,
+                    scope='resnet18_nvidia'):
+  """ResNet-18 optimized by Nvidia TensorRT."""
+  if multi_grid is None:
+    multi_grid = _DEFAULT_MULTI_GRID
+  else:
+    if len(multi_grid) != 3:
+      raise ValueError('Expect multi_grid to have length 3.')
+  blocks = [
+      resnet18_nvidia_block(
+          'block1', base_depth=[64,64], num_units=1, stride=1, first_block=True),
+      resnet18_nvidia_block(
+          'block2', base_depth=[64,128], num_units=2, stride=2),
+      resnet18_nvidia_block(
+          'block3', base_depth=[128,256], num_units=2, stride=2),
+      resnet18_nvidia_block(
+          'block4', base_depth=[256,512], num_units=2, stride=2),
+      resnet18_nvidia_block(
+          'block5', base_depth=[512], num_units=1, stride=1),
+  ]
+  return resnet_v1_beta(
+      inputs,
+      blocks=blocks,
+      num_classes=num_classes,
+      is_training=is_training,
+      global_pool=global_pool,
+      output_stride=output_stride,
+      reuse=reuse,
+      scope=scope)
+
+
 def resnet_v1_50(inputs,
                  num_classes=None,
                  is_training=None,
diff --git a/research/deeplab/datasets/build_kitti_lane_data.py b/research/deeplab/datasets/build_kitti_lane_data.py
new file mode 100644
index 00000000..8e73223a
--- /dev/null
+++ b/research/deeplab/datasets/build_kitti_lane_data.py
@@ -0,0 +1,149 @@
+# Copyright 2018 The TensorFlow Authors All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""Converts Kitti Lane data to TFRecord file format with Example protos."""
+import glob
+import math
+import os
+import re
+import sys
+import build_data
+import tensorflow as tf
+
+
+FLAGS = tf.app.flags.FLAGS
+
+tf.app.flags.DEFINE_string('kitti_lane_data_root',
+                           '/home/user/data/processed_data_road',
+                           'Kitti_Lane dataset root folder.')
+
+tf.app.flags.DEFINE_string(
+    'output_dir',
+    '/home/user/data/tfrecords',
+    'Path to save converted SSTable of TensorFlow examples.')
+
+
+
+_NUM_SHARDS = 1
+
+# A map from data type to folder name that saves the data.
+_FOLDERS_MAP = {
+    'image': 'image',
+    'label': 'gt_image',
+}
+
+# A map from data type to filename postfix.
+_POSTFIX_MAP = {
+    'image': '',
+    'label': '',
+}
+
+# A map from data type to data format.
+_DATA_FORMAT_MAP = {
+    'image': 'jpeg',
+    'label': 'png',
+}
+
+# Image file pattern.
+_IMAGE_FILENAME_RE = re.compile('(.+)' + _POSTFIX_MAP['image'])
+
+
+
+
+def _get_files(data, dataset_split):
+  """Gets files for the specified data type and dataset split.
+
+  Args:
+    data: String, desired data ('image' or 'label').
+    dataset_split: String, dataset split ('train', 'val', 'test')
+
+  Returns:
+    A list of sorted file names or None when getting label for
+      test set.
+  """
+  if data == 'label' and dataset_split == 'test':
+    return None
+
+  pattern = '*%s.%s' % (_POSTFIX_MAP[data], _DATA_FORMAT_MAP[data])
+  search_files = os.path.join(FLAGS.kitti_lane_data_root, _FOLDERS_MAP[data], dataset_split, pattern)
+  #search_files = os.path.join(FLAGS.kitti_lane_data_root, _FOLDERS_MAP[data], pattern)
+  filenames = glob.glob(search_files)
+  return sorted(filenames)
+
+
+def _convert_dataset(dataset_split):
+  """Converts the specified dataset split to TFRecord format.
+
+  Args:
+    dataset_split: The dataset split (e.g., train, val).
+
+  Raises:
+    RuntimeError: If loaded image and label have different shape, or if the
+      image file with specified postfix could not be found.
+  """
+  image_files = _get_files('image', dataset_split)
+  label_files = _get_files('label', dataset_split)
+
+  num_images = len(image_files)
+  num_per_shard = int(math.ceil(num_images / float(_NUM_SHARDS)))
+
+  image_reader = build_data.ImageReader('jpeg', channels=3)
+  label_reader = build_data.ImageReader('png', channels=1)
+
+  for shard_id in range(_NUM_SHARDS):
+    shard_filename = '%s-%05d-of-%05d.tfrecord' % (
+        dataset_split, shard_id, _NUM_SHARDS)
+    output_filename = os.path.join(FLAGS.output_dir, shard_filename)
+    with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:
+      start_idx = shard_id * num_per_shard
+      end_idx = min((shard_id + 1) * num_per_shard, num_images)
+      for i in range(start_idx, end_idx):
+        sys.stdout.write('\r>> Converting image %d/%d shard %d' % (
+            i + 1, num_images, shard_id))
+        sys.stdout.flush()
+        # Read the image.
+        image_data = tf.gfile.FastGFile(image_files[i], 'rb').read()
+        height, width = image_reader.read_image_dims(image_data)
+        # Read the semantic segmentation annotation.
+        seg_data = tf.gfile.FastGFile(label_files[i], 'rb').read()
+        seg_height, seg_width = label_reader.read_image_dims(seg_data)
+
+        if height != seg_height or width != seg_width:
+          raise RuntimeError('Shape mismatched between image and label.')
+        # Convert to tf example.
+        re_match = _IMAGE_FILENAME_RE.search(image_files[i])
+        if re_match is None:
+          raise RuntimeError('Invalid image filename: ' + image_files[i])
+        filename = os.path.basename(re_match.group(1))
+        example = build_data.image_seg_to_tfexample(
+            image_data, filename, height, width, seg_data)
+        tfrecord_writer.write(example.SerializeToString())
+    sys.stdout.write('\n')
+    sys.stdout.flush()
+
+
+def main(unused_argv):
+  # Only support converting 'train' and 'val' sets for now.
+
+  # create a new directory for tfrecords.
+  if not os.path.exists(FLAGS.output_dir):
+        os.makedirs(os.path.join(FLAGS.output_dir))
+
+  for dataset_split in ['train','val']:
+    _convert_dataset(dataset_split)
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/deeplab/datasets/build_lanenet_data.py b/research/deeplab/datasets/build_lanenet_data.py
new file mode 100644
index 00000000..fcca07c5
--- /dev/null
+++ b/research/deeplab/datasets/build_lanenet_data.py
@@ -0,0 +1,138 @@
+
+"""Converts LaneNet dataset to TFRecord file format with Example protos.
+
+The Example proto contains the following fields:
+
+  image/encoded: encoded image content.
+  image/filename: image filename.
+  image/format: image file format.
+  image/height: image height.
+  image/width: image width.
+  image/channels: image channels.
+  image/segmentation/class/encoded: encoded semantic segmentation content.
+  image/segmentation/class/format: semantic segmentation file format.
+"""
+import glob
+import math
+import os.path
+import re
+import sys
+import build_data
+import tensorflow as tf
+
+FLAGS = tf.app.flags.FLAGS
+
+tf.app.flags.DEFINE_string('lanenet_data_root',
+                           '/home/adlsa_repo/datasets/LaneNet_dataset',
+                           'LaneNet dataset root folder.')
+
+tf.app.flags.DEFINE_string(
+    'output_dir',
+    '/home/adlsa_repo/datasets/LaneNet_dataset/tfrecords',
+    'Path to save converted SSTable of TensorFlow examples.')
+
+
+_NUM_SHARDS = 1
+
+# A map from data type to folder name that saves the data.
+_FOLDERS_MAP = {
+    'image': 'images',
+    'label': 'labels',
+}
+
+# A map from data type to filename postfix.
+_POSTFIX_MAP = {
+    'image': '',
+    'label': '',
+}
+
+# A map from data type to data format.
+_DATA_FORMAT_MAP = {
+    'image': 'jpeg',
+    'label': 'png',
+}
+
+# Image file pattern.
+_IMAGE_FILENAME_RE = re.compile('(.+)' + _POSTFIX_MAP['image'])
+
+
+def _get_files(data, dataset_split):
+  """Gets files for the specified data type and dataset split.
+
+  Args:
+    data: String, desired data ('image' or 'label').
+    dataset_split: String, dataset split ('train', 'val', 'test')
+
+  Returns:
+    A list of sorted file names or None when getting label for
+      test set.
+  """
+  if data == 'label' and dataset_split == 'test':
+    return None
+
+  pattern = '*%s.%s' % (_POSTFIX_MAP[data], _DATA_FORMAT_MAP[data])
+  search_files = os.path.join(FLAGS.lanenet_data_root, _FOLDERS_MAP[data], dataset_split, '*', pattern)
+  search_files = os.path.join(FLAGS.lanenet_data_root, _FOLDERS_MAP[data], pattern)
+  filenames = glob.glob(search_files)
+  return sorted(filenames)
+
+
+def _convert_dataset(dataset_split):
+  """Converts the specified dataset split to TFRecord format.
+
+  Args:
+    dataset_split: The dataset split (e.g., train, val).
+
+  Raises:
+    RuntimeError: If loaded image and label have different shape, or if the
+      image file with specified postfix could not be found.
+  """
+  image_files = _get_files('image', dataset_split)
+  label_files = _get_files('label', dataset_split)
+
+
+  num_images = len(image_files)
+  num_per_shard = int(math.ceil(num_images / float(_NUM_SHARDS)))
+
+  image_reader = build_data.ImageReader('jpeg', channels=3)
+  label_reader = build_data.ImageReader('png', channels=1)
+
+  for shard_id in range(_NUM_SHARDS):
+    shard_filename = '%s-%05d-of-%05d.tfrecord' % (
+        dataset_split, shard_id, _NUM_SHARDS)
+    output_filename = os.path.join(FLAGS.output_dir, shard_filename)
+    with tf.python_io.TFRecordWriter(output_filename) as tfrecord_writer:
+      start_idx = shard_id * num_per_shard
+      end_idx = min((shard_id + 1) * num_per_shard, num_images)
+      for i in range(start_idx, end_idx):
+        sys.stdout.write('\r>> Converting image %d/%d shard %d' % (
+            i + 1, num_images, shard_id))
+        sys.stdout.flush()
+        # Read the image.
+        image_data = tf.gfile.FastGFile(image_files[i], 'rb').read()
+        height, width = image_reader.read_image_dims(image_data)
+        # Read the semantic segmentation annotation.
+        seg_data = tf.gfile.FastGFile(label_files[i], 'rb').read()
+        seg_height, seg_width = label_reader.read_image_dims(seg_data)
+        if height != seg_height or width != seg_width:
+          raise RuntimeError('Shape mismatched between image and label.')
+        # Convert to tf example.
+        re_match = _IMAGE_FILENAME_RE.search(image_files[i])
+        if re_match is None:
+          raise RuntimeError('Invalid image filename: ' + image_files[i])
+        filename = os.path.basename(re_match.group(1))
+        example = build_data.image_seg_to_tfexample(
+            image_data, filename, height, width, seg_data)
+        tfrecord_writer.write(example.SerializeToString())
+    sys.stdout.write('\n')
+    sys.stdout.flush()
+
+
+def main(unused_argv):
+  # Only support converting 'train' and 'val' sets for now.
+  for dataset_split in ['train']: # ['train', 'val']:
+    _convert_dataset(dataset_split)
+
+
+if __name__ == '__main__':
+  tf.app.run()
diff --git a/research/deeplab/datasets/process_kitti_lane.py b/research/deeplab/datasets/process_kitti_lane.py
new file mode 100644
index 00000000..e116111b
--- /dev/null
+++ b/research/deeplab/datasets/process_kitti_lane.py
@@ -0,0 +1,64 @@
+# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+"""Process KITTI Lane data for creating tfrecords."""
+import os
+import argparse
+import glob
+import numpy as np
+from PIL import Image
+
+def main():
+    """Main"""
+    parser = argparse.ArgumentParser(description="Organize and process Kitti Road Data.")
+    parser.add_argument("-i", "--raw_data_dir", type=str, required=True, help="""Raw input path.""")
+    parser.add_argument("-o", "--processed_data_dir", type=str, required=True, help="""Processed output path.""")
+    parser.add_argument("-n", "--num_val_images", type=int, required=False, default=15, help="""Number of validation data.""")
+    args, _ = parser.parse_known_args()
+
+    gt_image_dir = args.processed_data_dir + "/gt_image"
+    image_dir = args.processed_data_dir + "/image"
+
+    if not os.path.exists(args.processed_data_dir):
+        os.makedirs(os.path.join(args.processed_data_dir, "gt_image", "train"))
+        os.makedirs(os.path.join(args.processed_data_dir, "gt_image", "val"))
+        os.makedirs(os.path.join(args.processed_data_dir, "image", "train"))
+        os.makedirs(os.path.join(args.processed_data_dir, "image", "val"))
+
+    gt_data = glob.glob(os.path.join(args.raw_data_dir, "training/gt_image_2/um_lane*.png"))
+    image_data = glob.glob(os.path.join(args.raw_data_dir, "training/image_2/um_*.png"))
+
+    gt_data.sort()
+    image_data.sort()
+
+    num_images = len(gt_data)
+
+    val_index = np.random.choice(num_images, size=args.num_val_images, replace=False, p=None)
+
+
+    for i in range(num_images):
+        image_gt_path = gt_data[i]
+        im_gt = np.asarray(Image.open(image_gt_path))
+        im_gt.flags.writeable = True
+        lane_h, lane_w = np.where(im_gt[:, :, 2] == 255)
+        ignore_h, ignore_w = np.where(im_gt[:, :, 0] == 0)
+        im_gt[lane_h, lane_w, 2] = 1
+        im_gt[ignore_h, ignore_w, 2] = 255
+        im_gt_processed = Image.fromarray(im_gt[:, :, 2], 'L')
+        basename = os.path.basename(image_gt_path)
+        filename, _ = os.path.splitext(basename)
+        if i in val_index:
+            im_gt_processed.save(os.path.join(gt_image_dir, 'val', filename + '.png'))
+        else:
+            im_gt_processed.save(os.path.join(gt_image_dir, 'train', filename + '.png'))
+
+        image_path = image_data[i]
+        im = Image.open(image_path)
+        basename = os.path.basename(image_path)
+        filename, _ = os.path.splitext(basename)
+        if i in val_index:
+            im.save(os.path.join(image_dir, 'val', filename + '.jpeg'))
+        else:
+            im.save(os.path.join(image_dir, 'train', filename + '.jpeg'))
+
+
+if __name__ == '__main__':
+    main()
diff --git a/research/deeplab/datasets/segmentation_dataset.py b/research/deeplab/datasets/segmentation_dataset.py
index 65c06041..3b51344d 100644
--- a/research/deeplab/datasets/segmentation_dataset.py
+++ b/research/deeplab/datasets/segmentation_dataset.py
@@ -108,11 +108,32 @@ _ADE20K_INFORMATION = DatasetDescriptor(
     ignore_label=0,
 )
 
+# Nvidia LaneNet dataset
+_LANENET_INFORMATION = DatasetDescriptor(
+    splits_to_sizes={
+        'train': 330000,
+        'val': 9884,
+    },
+    num_classes=4,
+    ignore_label=255,
+)
+
+# Kitti Lane dataset
+_KITTI_LANE_INFORMATION = DatasetDescriptor(
+    splits_to_sizes={
+        'train': 80,
+        'val': 15,
+    },
+    num_classes=2,
+    ignore_label=255,
+)
 
 _DATASETS_INFORMATION = {
     'cityscapes': _CITYSCAPES_INFORMATION,
     'pascal_voc_seg': _PASCAL_VOC_SEG_INFORMATION,
     'ade20k': _ADE20K_INFORMATION,
+    'lanenet_datasets': _LANENET_INFORMATION,
+    'kitti_lane_datasets':_KITTI_LANE_INFORMATION,
 }
 
 # Default file pattern of TFRecord of TensorFlow Example.
diff --git a/research/deeplab/export_model_nvidia.py b/research/deeplab/export_model_nvidia.py
new file mode 100644
index 00000000..9c20bd72
--- /dev/null
+++ b/research/deeplab/export_model_nvidia.py
@@ -0,0 +1,110 @@
+# Copyright (c) 2018, NVIDIA CORPORATION. All rights reserved.
+
+"""Exports TensorRT friendly models."""
+
+import os
+import tensorflow as tf
+
+#from tensorflow.python.tools import freeze_graph
+from deeplab import common
+from deeplab import input_preprocess
+from deeplab import model
+from deeplab import freeze_graph
+
+slim = tf.contrib.slim
+flags = tf.app.flags
+
+FLAGS = flags.FLAGS
+
+flags.DEFINE_string('checkpoint_path', None, 'Checkpoint path')
+
+flags.DEFINE_string('export_path', None,
+                    'Path to output Tensorflow frozen graph.')
+
+flags.DEFINE_integer('num_classes', 21, 'Number of classes.')
+
+flags.DEFINE_multi_integer('crop_size', [513, 513],
+                           'Crop size [height, width].')
+
+
+# Dilated rate should be smaller than 16 to fit nvidia TensorRT DLA constraints.
+# We can set rates = [6, 10, 14]
+flags.DEFINE_multi_integer('atrous_rates', None,
+                           'Atrous rates for atrous spatial pyramid pooling.')
+
+flags.DEFINE_integer('output_stride', 8,
+                     'The ratio of input to output spatial resolution.')
+
+# Change to [0.5, 0.75, 1.0, 1.25, 1.5, 1.75] for multi-scale inference.
+# We don't perform inference_scales other than [1.0] as TensorRT doesn't support
+# tf.ResizeBilinear operation.
+flags.DEFINE_multi_float('inference_scales', [1.0],
+                         'The scales to resize images for inference.')
+
+flags.DEFINE_bool('add_flipped_images', False,
+                  'Add flipped images during inference or not.')
+
+# Input name of the exported model.
+_INPUT_NAME = 'ImageTensor'
+
+# Output name of the exported model.
+_OUTPUT_NAME = 'SemanticPredictions'
+
+
+def main(unused_argv):
+  tf.logging.set_verbosity(tf.logging.INFO)
+  tf.logging.info('Prepare to export model to: %s', FLAGS.export_path)
+
+  with tf.Graph().as_default():
+    with tf.Session(config=tf.ConfigProto()) as tf_sess:
+
+      # Remove _create_input_tensors method from original export_model.py to neglect the unnecessary operations in TensorRT.
+      # Specify fixed input dimension in placeholder
+      image = tf.placeholder(tf.float32, [1, FLAGS.crop_size[0], FLAGS.crop_size[1], 3], name=_INPUT_NAME)
+
+      model_options = common.ModelOptions(
+          outputs_to_num_classes={common.OUTPUT_TYPE: FLAGS.num_classes},
+          crop_size=FLAGS.crop_size,
+          atrous_rates=FLAGS.atrous_rates,
+          output_stride=FLAGS.output_stride)
+
+      if tuple(FLAGS.inference_scales) == (1.0,):
+        print('Exported model performs single-scale inference.')
+        tf.logging.info('Exported model performs single-scale inference.')
+        predictions = model.predict_labels(image,model_options=model_options,image_pyramid=FLAGS.image_pyramid, is_exporting=True)
+      else:
+        print('Exported model performs multi-scale inference.')
+        tf.logging.info('Exported model performs multi-scale inference.')
+        predictions = model.predict_labels_multi_scale(
+            image,
+            model_options=model_options,
+            eval_scales=FLAGS.inference_scales,
+            add_flipped_images=FLAGS.add_flipped_images)
+
+      semantic_predictions = predictions['merged_logits']
+
+      # Remove _resize_label method and slice operations from original export_model.py
+      semantic_predictions = tf.identity(semantic_predictions, name=_OUTPUT_NAME)
+
+
+      saver = tf.train.Saver(tf.model_variables())
+
+      tf.gfile.MakeDirs(os.path.dirname(FLAGS.export_path))
+
+      freeze_graph.freeze_graph_with_def_protos(
+          tf.get_default_graph().as_graph_def(add_shapes=True),
+          saver.as_saver_def(),
+          FLAGS.checkpoint_path,
+          _OUTPUT_NAME,
+          restore_op_name=None,
+          filename_tensor_name=None,
+          output_graph=FLAGS.export_path,
+          clear_devices=True,
+          initializer_nodes=None)
+
+
+
+if __name__ == '__main__':
+  flags.mark_flag_as_required('checkpoint_path')
+  flags.mark_flag_as_required('export_path')
+  tf.app.run()
diff --git a/research/deeplab/freeze_graph.py b/research/deeplab/freeze_graph.py
new file mode 100644
index 00000000..c95a550d
--- /dev/null
+++ b/research/deeplab/freeze_graph.py
@@ -0,0 +1,413 @@
+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+r"""Converts checkpoint variables into Const ops in a standalone GraphDef file.
+
+This script is designed to take a GraphDef proto, a SaverDef proto, and a set of
+variable values stored in a checkpoint file, and output a GraphDef with all of
+the variable ops converted into const ops containing the values of the
+variables.
+
+It's useful to do this when we need to load a single file in C++, especially in
+environments like mobile or embedded where we may not have access to the
+RestoreTensor ops and file loading calls that they rely on.
+
+An example of command-line usage is:
+bazel build tensorflow/python/tools:freeze_graph && \
+bazel-bin/tensorflow/python/tools/freeze_graph \
+--input_graph=some_graph_def.pb \
+--input_checkpoint=model.ckpt-8361242 \
+--output_graph=/tmp/frozen_graph.pb --output_node_names=softmax
+
+You can also look at freeze_graph_test.py for an example of how to use it.
+
+"""
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+
+import argparse
+import re
+import sys
+
+from google.protobuf import text_format
+
+from tensorflow.core.framework import graph_pb2
+from tensorflow.core.protobuf import saver_pb2
+from tensorflow.core.protobuf.meta_graph_pb2 import MetaGraphDef
+from tensorflow.python import pywrap_tensorflow
+from tensorflow.python.client import session
+from tensorflow.python.framework import graph_util
+from tensorflow.python.framework import importer
+from tensorflow.python.platform import app
+from tensorflow.python.platform import gfile
+from tensorflow.python.saved_model import loader
+from tensorflow.python.saved_model import tag_constants
+from tensorflow.python.tools import saved_model_utils
+from tensorflow.python.training import saver as saver_lib
+import tensorflow as tf
+
+
+def freeze_graph_with_def_protos(input_graph_def,
+                                 input_saver_def,
+                                 input_checkpoint,
+                                 output_node_names,
+                                 restore_op_name,
+                                 filename_tensor_name,
+                                 output_graph,
+                                 clear_devices,
+                                 initializer_nodes,
+                                 variable_names_whitelist="",
+                                 variable_names_blacklist="",
+                                 input_meta_graph_def=None,
+                                 input_saved_model_dir=None,
+                                 saved_model_tags=None,
+                                 checkpoint_version=saver_pb2.SaverDef.V2):
+  """Converts all variables in a graph and checkpoint into constants."""
+  del restore_op_name, filename_tensor_name  # Unused by updated loading code.
+
+  # 'input_checkpoint' may be a prefix if we're using Saver V2 format
+  if (not input_saved_model_dir and
+      not saver_lib.checkpoint_exists(input_checkpoint)):
+    print("Input checkpoint '" + input_checkpoint + "' doesn't exist!")
+    return -1
+
+  if not output_node_names:
+    print("You need to supply the name of a node to --output_node_names.")
+    return -1
+
+  # Remove all the explicit device specifications for this node. This helps to
+  # make the graph more portable.
+  if clear_devices:
+    if input_meta_graph_def:
+      for node in input_meta_graph_def.graph_def.node:
+        node.device = ""
+    elif input_graph_def:
+      for node in input_graph_def.node:
+        node.device = ""
+
+  if input_graph_def:
+    _ = importer.import_graph_def(input_graph_def, name="")
+  with session.Session() as sess:
+    if input_saver_def:
+      saver = saver_lib.Saver(
+          saver_def=input_saver_def, write_version=checkpoint_version)
+      saver.restore(sess, input_checkpoint)
+    elif input_meta_graph_def:
+      restorer = saver_lib.import_meta_graph(
+          input_meta_graph_def, clear_devices=True)
+      restorer.restore(sess, input_checkpoint)
+      if initializer_nodes:
+        sess.run(initializer_nodes.replace(" ", "").split(","))
+    elif input_saved_model_dir:
+      if saved_model_tags is None:
+        saved_model_tags = []
+      loader.load(sess, saved_model_tags, input_saved_model_dir)
+    else:
+      var_list = {}
+      reader = pywrap_tensorflow.NewCheckpointReader(input_checkpoint)
+      var_to_shape_map = reader.get_variable_to_shape_map()
+
+      # List of all partition variables. Because the condition is heuristic
+      # based, the list could include false positives.
+      all_parition_variable_names = [
+          tensor.name.split(":")[0]
+          for op in sess.graph.get_operations()
+          for tensor in op.values()
+          if re.search(r"/part_\d+/", tensor.name)
+      ]
+      has_partition_var = False
+
+      for key in var_to_shape_map:
+        try:
+          tensor = sess.graph.get_tensor_by_name(key + ":0")
+          if any(key in name for name in all_parition_variable_names):
+            has_partition_var = True
+        except KeyError:
+          # This tensor doesn't exist in the graph (for example it's
+          # 'global_step' or a similar housekeeping element) so skip it.
+          continue
+        var_list[key] = tensor
+
+      try:
+        saver = saver_lib.Saver(
+            var_list=var_list, write_version=checkpoint_version)
+      except TypeError as e:
+        # `var_list` is required to be a map of variable names to Variable
+        # tensors. Partition variables are Identity tensors that cannot be
+        # handled by Saver.
+        if has_partition_var:
+          print("Models containing partition variables cannot be converted "
+                "from checkpoint files. Please pass in a SavedModel using "
+                "the flag --input_saved_model_dir.")
+          return -1
+        else:
+          raise e
+
+      saver.restore(sess, input_checkpoint)
+      if initializer_nodes:
+        sess.run(initializer_nodes.replace(" ", "").split(","))
+
+    variable_names_whitelist = (
+        variable_names_whitelist.replace(" ", "").split(",")
+        if variable_names_whitelist else None)
+    variable_names_blacklist = (
+        variable_names_blacklist.replace(" ", "").split(",")
+        if variable_names_blacklist else None)
+
+    if input_meta_graph_def:
+      output_graph_def = graph_util.convert_variables_to_constants(
+          sess,
+          input_meta_graph_def.graph_def,
+          output_node_names.replace(" ", "").split(","),
+          variable_names_whitelist=variable_names_whitelist,
+          variable_names_blacklist=variable_names_blacklist)
+    else:
+      output_graph_def = graph_util.convert_variables_to_constants(
+          sess,
+          input_graph_def,
+          output_node_names.replace(" ", "").split(","),
+          variable_names_whitelist=variable_names_whitelist,
+          variable_names_blacklist=variable_names_blacklist)
+
+    output_graph_def = tf.graph_util.remove_training_nodes(output_graph_def)
+
+  # Write GraphDef to file if output path has been given.
+  if output_graph:
+    with gfile.GFile(output_graph, "wb") as f:
+      f.write(output_graph_def.SerializeToString())
+
+  return output_graph_def
+
+
+def _parse_input_graph_proto(input_graph, input_binary):
+  """Parser input tensorflow graph into GraphDef proto."""
+  if not gfile.Exists(input_graph):
+    print("Input graph file '" + input_graph + "' does not exist!")
+    return -1
+  input_graph_def = graph_pb2.GraphDef()
+  mode = "rb" if input_binary else "r"
+  with gfile.FastGFile(input_graph, mode) as f:
+    if input_binary:
+      input_graph_def.ParseFromString(f.read())
+    else:
+      text_format.Merge(f.read(), input_graph_def)
+  return input_graph_def
+
+
+def _parse_input_meta_graph_proto(input_graph, input_binary):
+  """Parser input tensorflow graph into MetaGraphDef proto."""
+  if not gfile.Exists(input_graph):
+    print("Input meta graph file '" + input_graph + "' does not exist!")
+    return -1
+  input_meta_graph_def = MetaGraphDef()
+  mode = "rb" if input_binary else "r"
+  with gfile.FastGFile(input_graph, mode) as f:
+    if input_binary:
+      input_meta_graph_def.ParseFromString(f.read())
+    else:
+      text_format.Merge(f.read(), input_meta_graph_def)
+  print("Loaded meta graph file '" + input_graph)
+  return input_meta_graph_def
+
+
+def _parse_input_saver_proto(input_saver, input_binary):
+  """Parser input tensorflow Saver into SaverDef proto."""
+  if not gfile.Exists(input_saver):
+    print("Input saver file '" + input_saver + "' does not exist!")
+    return -1
+  mode = "rb" if input_binary else "r"
+  with gfile.FastGFile(input_saver, mode) as f:
+    saver_def = saver_pb2.SaverDef()
+    if input_binary:
+      saver_def.ParseFromString(f.read())
+    else:
+      text_format.Merge(f.read(), saver_def)
+  return saver_def
+
+
+def freeze_graph(input_graph,
+                 input_saver,
+                 input_binary,
+                 input_checkpoint,
+                 output_node_names,
+                 restore_op_name,
+                 filename_tensor_name,
+                 output_graph,
+                 clear_devices,
+                 initializer_nodes,
+                 variable_names_whitelist="",
+                 variable_names_blacklist="",
+                 input_meta_graph=None,
+                 input_saved_model_dir=None,
+                 saved_model_tags=tag_constants.SERVING,
+                 checkpoint_version=saver_pb2.SaverDef.V2):
+  """Converts all variables in a graph and checkpoint into constants."""
+  input_graph_def = None
+  if input_saved_model_dir:
+    input_graph_def = saved_model_utils.get_meta_graph_def(
+        input_saved_model_dir, saved_model_tags).graph_def
+  elif input_graph:
+    input_graph_def = _parse_input_graph_proto(input_graph, input_binary)
+  input_meta_graph_def = None
+  if input_meta_graph:
+    input_meta_graph_def = _parse_input_meta_graph_proto(
+        input_meta_graph, input_binary)
+  input_saver_def = None
+  if input_saver:
+    input_saver_def = _parse_input_saver_proto(input_saver, input_binary)
+  freeze_graph_with_def_protos(
+      input_graph_def,
+      input_saver_def,
+      input_checkpoint,
+      output_node_names,
+      restore_op_name,
+      filename_tensor_name,
+      output_graph,
+      clear_devices,
+      initializer_nodes,
+      variable_names_whitelist,
+      variable_names_blacklist,
+      input_meta_graph_def,
+      input_saved_model_dir,
+      saved_model_tags.replace(" ", "").split(","),
+      checkpoint_version=checkpoint_version)
+
+
+def main(unused_args, flags):
+  if flags.checkpoint_version == 1:
+    checkpoint_version = saver_pb2.SaverDef.V1
+  elif flags.checkpoint_version == 2:
+    checkpoint_version = saver_pb2.SaverDef.V2
+  else:
+    print("Invalid checkpoint version (must be '1' or '2'): %d" %
+          flags.checkpoint_version)
+    return -1
+  freeze_graph(flags.input_graph, flags.input_saver, flags.input_binary,
+               flags.input_checkpoint, flags.output_node_names,
+               flags.restore_op_name, flags.filename_tensor_name,
+               flags.output_graph, flags.clear_devices, flags.initializer_nodes,
+               flags.variable_names_whitelist, flags.variable_names_blacklist,
+               flags.input_meta_graph, flags.input_saved_model_dir,
+               flags.saved_model_tags, checkpoint_version)
+
+def run_main():
+  parser = argparse.ArgumentParser()
+  parser.register("type", "bool", lambda v: v.lower() == "true")
+  parser.add_argument(
+      "--input_graph",
+      type=str,
+      default="",
+      help="TensorFlow \'GraphDef\' file to load.")
+  parser.add_argument(
+      "--input_saver",
+      type=str,
+      default="",
+      help="TensorFlow saver file to load.")
+  parser.add_argument(
+      "--input_checkpoint",
+      type=str,
+      default="",
+      help="TensorFlow variables file to load.")
+  parser.add_argument(
+      "--checkpoint_version",
+      type=int,
+      default=2,
+      help="Tensorflow variable file format")
+  parser.add_argument(
+      "--output_graph",
+      type=str,
+      default="",
+      help="Output \'GraphDef\' file name.")
+  parser.add_argument(
+      "--input_binary",
+      nargs="?",
+      const=True,
+      type="bool",
+      default=False,
+      help="Whether the input files are in binary format.")
+  parser.add_argument(
+      "--output_node_names",
+      type=str,
+      default="",
+      help="The name of the output nodes, comma separated.")
+  parser.add_argument(
+      "--restore_op_name",
+      type=str,
+      default="save/restore_all",
+      help="""\
+      The name of the master restore operator. Deprecated, unused by updated \
+      loading code.
+      """)
+  parser.add_argument(
+      "--filename_tensor_name",
+      type=str,
+      default="save/Const:0",
+      help="""\
+      The name of the tensor holding the save path. Deprecated, unused by \
+      updated loading code.
+      """)
+  parser.add_argument(
+      "--clear_devices",
+      nargs="?",
+      const=True,
+      type="bool",
+      default=True,
+      help="Whether to remove device specifications.")
+  parser.add_argument(
+      "--initializer_nodes",
+      type=str,
+      default="",
+      help="Comma separated list of initializer nodes to run before freezing.")
+  parser.add_argument(
+      "--variable_names_whitelist",
+      type=str,
+      default="",
+      help="""\
+      Comma separated list of variables to convert to constants. If specified, \
+      only those variables will be converted to constants.\
+      """)
+  parser.add_argument(
+      "--variable_names_blacklist",
+      type=str,
+      default="",
+      help="""\
+      Comma separated list of variables to skip converting to constants.\
+      """)
+  parser.add_argument(
+      "--input_meta_graph",
+      type=str,
+      default="",
+      help="TensorFlow \'MetaGraphDef\' file to load.")
+  parser.add_argument(
+      "--input_saved_model_dir",
+      type=str,
+      default="",
+      help="Path to the dir with TensorFlow \'SavedModel\' file and variables.")
+  parser.add_argument(
+      "--saved_model_tags",
+      type=str,
+      default="serve",
+      help="""\
+      Group of tag(s) of the MetaGraphDef to load, in string format,\
+      separated by \',\'. For tag-set contains multiple tags, all tags \
+      must be passed in.\
+      """)
+  flags, unparsed = parser.parse_known_args()
+
+  my_main = lambda unused_args: main(unused_args, flags)
+  app.run(main=my_main, argv=[sys.argv[0]] + unparsed)
+
+if __name__ == '__main__':
+  run_main()
diff --git a/research/deeplab/model.py b/research/deeplab/model.py
index 8a687395..19e9c260 100644
--- a/research/deeplab/model.py
+++ b/research/deeplab/model.py
@@ -161,13 +161,14 @@ def predict_labels_multi_scale(images,
   return outputs_to_predictions
 
 
-def predict_labels(images, model_options, image_pyramid=None):
+def predict_labels(images, model_options, image_pyramid=None, is_exporting=False):
   """Predicts segmentation labels.
 
   Args:
     images: A tensor of size [batch, height, width, channels].
     model_options: A ModelOptions instance to configure models.
     image_pyramid: Input image scales for multi-scale feature extraction.
+    is_exporting: Is exporting or not. This flag is for TensorRT
 
   Returns:
     A dictionary with keys specifying the output_type (e.g., semantic
@@ -179,18 +180,26 @@ def predict_labels(images, model_options, image_pyramid=None):
       model_options=model_options,
       image_pyramid=image_pyramid,
       is_training=False,
-      fine_tune_batch_norm=False)
+      fine_tune_batch_norm=False,
+      is_exporting=is_exporting)
 
   predictions = {}
   for output in sorted(outputs_to_scales_to_logits):
     scales_to_logits = outputs_to_scales_to_logits[output]
-    logits = tf.image.resize_bilinear(
-        scales_to_logits[MERGED_LOGITS_SCOPE],
-        tf.shape(images)[1:3],
-        align_corners=True)
-    predictions[output] = tf.argmax(logits, 3)
 
-  return predictions
+    # Remove unsupported operations when exporting model for TensorRT.
+    if is_exporting == False:
+      logits = tf.image.resize_bilinear(
+          scales_to_logits[MERGED_LOGITS_SCOPE],
+          tf.shape(images)[1:3],
+          align_corners=True)
+      predictions[output] = tf.argmax(logits, 3)
+
+  # Return scales_to_logits if is_exporting = True
+  if is_exporting == False:
+    return predictions
+  else:
+    return scales_to_logits
 
 
 def _resize_bilinear(images, size, output_dtype=tf.float32):
@@ -214,7 +223,8 @@ def multi_scale_logits(images,
                        image_pyramid,
                        weight_decay=0.0001,
                        is_training=False,
-                       fine_tune_batch_norm=False):
+                       fine_tune_batch_norm=False,
+                       is_exporting=False):
   """Gets the logits for multi-scale inputs.
 
   The returned logits are all downsampled (due to max-pooling layers)
@@ -227,6 +237,7 @@ def multi_scale_logits(images,
     weight_decay: The weight decay for model variables.
     is_training: Is training or not.
     fine_tune_batch_norm: Fine-tune the batch norm parameters or not.
+    is_exporting: Is exporting or not. This flag is for TensorRT
 
   Returns:
     outputs_to_scales_to_logits: A map of maps from output_type (e.g.,
@@ -291,10 +302,13 @@ def multi_scale_logits(images,
         fine_tune_batch_norm=fine_tune_batch_norm)
 
     # Resize the logits to have the same dimension before merging.
-    for output in sorted(outputs_to_logits):
-      outputs_to_logits[output] = tf.image.resize_bilinear(
-          outputs_to_logits[output], [logits_height, logits_width],
-          align_corners=True)
+    # Remove tf.Resize_Bilinear operations when exporting models as
+    # TensorRT doesn't support this operation.
+    if is_exporting == False:
+      for output in sorted(outputs_to_logits):
+        outputs_to_logits[output] = tf.image.resize_bilinear(
+            outputs_to_logits[output], [logits_height, logits_width],
+            align_corners=True)
 
     # Return when only one input scale.
     if len(image_pyramid) == 1:
@@ -400,7 +414,8 @@ def extract_features(images,
           stride=1,
           reuse=reuse):
         with slim.arg_scope([slim.batch_norm], **batch_norm_params):
-          depth = 256
+          # Modify depth from 256 to 196 to fit Nvidia TensorRT DLA computation constraints.
+          depth = 196
           branch_logits = []
 
           if model_options.add_image_level_feature:
diff --git a/research/deeplab/train.py b/research/deeplab/train.py
index bf362922..ffa7ad80 100644
--- a/research/deeplab/train.py
+++ b/research/deeplab/train.py
@@ -166,6 +166,8 @@ flags.DEFINE_string('train_split', 'train',
 
 flags.DEFINE_string('dataset_dir', None, 'Where the dataset reside.')
 
+# Set to True if loading pretrained weights from resnet18_nvidia
+flags.DEFINE_boolean('restore_resnet18_nvidia_checkpoint', False, 'Restore_resnet18_nvidia_checkpoint.')
 
 def _build_deeplab(inputs_queue, outputs_to_num_classes, ignore_label):
   """Builds a clone of DeepLab.
@@ -381,7 +383,8 @@ def main(unused_argv):
             FLAGS.tf_initial_checkpoint,
             FLAGS.initialize_last_layer,
             last_layers,
-            ignore_missing_vars=True),
+            ignore_missing_vars=True,
+            restore_resnet18_nvidia_checkpoint = FLAGS.restore_resnet18_nvidia_checkpoint),
         summary_op=summary_op,
         save_summaries_secs=FLAGS.save_summaries_secs,
         save_interval_secs=FLAGS.save_interval_secs)
@@ -389,6 +392,9 @@ def main(unused_argv):
 
 if __name__ == '__main__':
   flags.mark_flag_as_required('train_logdir')
-  flags.mark_flag_as_required('tf_initial_checkpoint')
   flags.mark_flag_as_required('dataset_dir')
+
+  # Comment it out to make models trained from scratch.
+  # flags.mark_flag_as_required('tf_initial_checkpoint')
+
   tf.app.run()
diff --git a/research/deeplab/utils/train_utils.py b/research/deeplab/utils/train_utils.py
index 4eeffb1e..d022ab6d 100644
--- a/research/deeplab/utils/train_utils.py
+++ b/research/deeplab/utils/train_utils.py
@@ -82,7 +82,8 @@ def get_model_init_fn(train_logdir,
                       tf_initial_checkpoint,
                       initialize_last_layer,
                       last_layers,
-                      ignore_missing_vars=False):
+                      ignore_missing_vars=False,
+                      restore_resnet18_nvidia_checkpoint=False):
   """Gets the function initializing model variables from a checkpoint.
 
   Args:
@@ -112,6 +113,18 @@ def get_model_init_fn(train_logdir,
 
   variables_to_restore = slim.get_variables_to_restore(exclude=exclude_list)
 
+  # Setup tf.variables that will be restored from resnet18_nvidia pretrained weights
+  # if restore_resnet18_nvidia_checkpoint == True
+  if restore_resnet18_nvidia_checkpoint == True:
+    variables_to_restore = {'FeatureExtractor/resnet18_nvidia' + var.name.strip('resnet18_nvidia_deeplab').strip(':0') : var
+                              for var in variables_to_restore
+                                if not var.name.endswith('Momentum:0')
+                                   and(var.name.startswith('resnet18_nvidia_deeplab/block1') or
+                                       var.name.startswith('resnet18_nvidia_deeplab/block1') or
+                                       var.name.startswith('resnet18_nvidia_deeplab/block2') or
+                                       var.name.startswith('resnet18_nvidia_deeplab/block3'))}
+    ignore_missing_vars = False
+
   if variables_to_restore:
     return slim.assign_from_checkpoint_fn(
         tf_initial_checkpoint,
diff --git a/research/deeplab/vis.py b/research/deeplab/vis.py
index 9f75d104..1dfc7188 100644
--- a/research/deeplab/vis.py
+++ b/research/deeplab/vis.py
@@ -309,8 +309,9 @@ def main(unused_argv):
           'Finished visualization at ' + time.strftime('%Y-%m-%d-%H:%M:%S',
                                                        time.gmtime()))
       time_to_next_eval = start + FLAGS.eval_interval_secs - time.time()
-      if time_to_next_eval > 0:
-        time.sleep(time_to_next_eval)
+    
+    #   if time_to_next_eval > 0:
+    #     time.sleep(time_to_next_eval)
 
 
 if __name__ == '__main__':
diff --git a/research/object_detection/builders/model_builder.py b/research/object_detection/builders/model_builder.py
index e754b376..cbb61d3c 100644
--- a/research/object_detection/builders/model_builder.py
+++ b/research/object_detection/builders/model_builder.py
@@ -37,6 +37,7 @@ from object_detection.models import faster_rcnn_inception_v2_feature_extractor a
 from object_detection.models import faster_rcnn_nas_feature_extractor as frcnn_nas
 from object_detection.models import faster_rcnn_pnas_feature_extractor as frcnn_pnas
 from object_detection.models import faster_rcnn_resnet_v1_feature_extractor as frcnn_resnet_v1
+from object_detection.models import ssd_resnet_v1_feature_extractor as ssd_resnet_v1
 from object_detection.models import ssd_resnet_v1_fpn_feature_extractor as ssd_resnet_v1_fpn
 from object_detection.models import ssd_resnet_v1_ppn_feature_extractor as ssd_resnet_v1_ppn
 from object_detection.models.embedded_ssd_mobilenet_v1_feature_extractor import EmbeddedSSDMobileNetV1FeatureExtractor
@@ -60,6 +61,10 @@ SSD_FEATURE_EXTRACTOR_CLASS_MAP = {
     'ssd_mobilenet_v1_ppn': SSDMobileNetV1PpnFeatureExtractor,
     'ssd_mobilenet_v2': SSDMobileNetV2FeatureExtractor,
     'ssd_mobilenet_v2_fpn': SSDMobileNetV2FpnFeatureExtractor,
+    'ssd_resnet18_v1': ssd_resnet_v1.SSDResnet18V1FeatureExtractor,
+    'ssd_resnet50_v1': ssd_resnet_v1.SSDResnet50V1FeatureExtractor,
+    'ssd_resnet101_v1': ssd_resnet_v1.SSDResnet101V1FeatureExtractor,
+    'ssd_resnet152_v1': ssd_resnet_v1.SSDResnet152V1FeatureExtractor,
     'ssd_resnet50_v1_fpn': ssd_resnet_v1_fpn.SSDResnet50V1FpnFeatureExtractor,
     'ssd_resnet101_v1_fpn': ssd_resnet_v1_fpn.SSDResnet101V1FpnFeatureExtractor,
     'ssd_resnet152_v1_fpn': ssd_resnet_v1_fpn.SSDResnet152V1FpnFeatureExtractor,
@@ -210,7 +215,8 @@ def _build_ssd_model(ssd_config, is_training, add_summaries,
   # Feature extractor
   feature_extractor = _build_ssd_feature_extractor(
       feature_extractor_config=ssd_config.feature_extractor,
-      is_training=is_training)
+      is_training=is_training,
+      reuse_weights=None)#(tf.AUTO_REUSE if 'ssd_resnet' in ssd_config.feature_extractor.type else None))
 
   box_coder = box_coder_builder.build(ssd_config.box_coder)
   matcher = matcher_builder.build(ssd_config.matcher)
diff --git a/research/object_detection/dataset_tools/create_coco_tf_record.py b/research/object_detection/dataset_tools/create_coco_tf_record.py
index 7f2bd1fb..39808e2f 100644
--- a/research/object_detection/dataset_tools/create_coco_tf_record.py
+++ b/research/object_detection/dataset_tools/create_coco_tf_record.py
@@ -64,6 +64,9 @@ tf.flags.DEFINE_string('val_annotations_file', '',
 tf.flags.DEFINE_string('testdev_annotations_file', '',
                        'Test-dev annotations JSON file.')
 tf.flags.DEFINE_string('output_dir', '/tmp/', 'Output data directory.')
+tf.flags.DEFINE_string('encoded_type', 'jpg', 'Image type to encode (jpg or png).')
+tf.flags.DEFINE_integer('image_save_period', '0', 'Save a JPEG image every'
+                        '`image_save_period` image file.')
 
 FLAGS = flags.FLAGS
 
@@ -74,6 +77,9 @@ def create_tf_example(image,
                       annotations_list,
                       image_dir,
                       category_index,
+                      encoded_type,
+                      save_image,
+                      image_output_path,
                       include_masks=False):
   """Converts image and annotations to a tf.Example proto.
 
@@ -114,7 +120,15 @@ def create_tf_example(image,
     encoded_jpg = fid.read()
   encoded_jpg_io = io.BytesIO(encoded_jpg)
   image = PIL.Image.open(encoded_jpg_io)
-  key = hashlib.sha256(encoded_jpg).hexdigest()
+  if encoded_type == 'png':
+    encoded_image = tf_record_creation_util.convert_to_png(image)
+    if save_image:
+      image.save(os.path.join(image_output_path, filename[:filename.rfind('.')] + '.png'))
+  elif encoded_type == 'jpeg':
+    encoded_image = tf_record_creation_util.convert_to_jpeg(image)
+    if save_image:
+      image.save(os.path.join(image_output_path, filename[:filename.rfind('.')] + '.jpeg'))
+  key = hashlib.sha256(encoded_image).hexdigest()
 
   xmin = []
   xmax = []
@@ -166,9 +180,9 @@ def create_tf_example(image,
       'image/key/sha256':
           dataset_util.bytes_feature(key.encode('utf8')),
       'image/encoded':
-          dataset_util.bytes_feature(encoded_jpg),
+          dataset_util.bytes_feature(encoded_image),
       'image/format':
-          dataset_util.bytes_feature('jpeg'.encode('utf8')),
+          dataset_util.bytes_feature(encoded_type.encode('utf8')),
       'image/object/bbox/xmin':
           dataset_util.float_list_feature(xmin),
       'image/object/bbox/xmax':
@@ -192,7 +206,8 @@ def create_tf_example(image,
 
 
 def _create_tf_record_from_coco_annotations(
-    annotations_file, image_dir, output_path, include_masks, num_shards):
+    annotations_file, image_dir, output_path, image_output_path, include_masks,
+    encoded_type, image_save_period, num_shards):
   """Loads COCO annotation json files and converts to tf.Record format.
 
   Args:
@@ -203,6 +218,9 @@ def _create_tf_record_from_coco_annotations(
       (PNG encoded) in the result. default: False.
     num_shards: number of output file shards.
   """
+  encoded_type = encoded_type.lower()
+  if encoded_type == 'jpg':
+    encoded_type = 'jpeg'
   with contextlib2.ExitStack() as tf_record_close_stack, \
       tf.gfile.GFile(annotations_file, 'r') as fid:
     output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(
@@ -235,8 +253,10 @@ def _create_tf_record_from_coco_annotations(
       if idx % 100 == 0:
         tf.logging.info('On image %d of %d', idx, len(images))
       annotations_list = annotations_index[image['id']]
+      save_image = False if image_save_period == 0 else idx % image_save_period == 0
       _, tf_example, num_annotations_skipped = create_tf_example(
-          image, annotations_list, image_dir, category_index, include_masks)
+          image, annotations_list, image_dir, category_index,
+          encoded_type, save_image, image_output_path, include_masks)
       total_num_annotations_skipped += num_annotations_skipped
       shard_idx = idx % num_shards
       output_tfrecords[shard_idx].write(tf_example.SerializeToString())
@@ -254,27 +274,49 @@ def main(_):
 
   if not tf.gfile.IsDirectory(FLAGS.output_dir):
     tf.gfile.MakeDirs(FLAGS.output_dir)
-  train_output_path = os.path.join(FLAGS.output_dir, 'coco_train.record')
-  val_output_path = os.path.join(FLAGS.output_dir, 'coco_val.record')
-  testdev_output_path = os.path.join(FLAGS.output_dir, 'coco_testdev.record')
+  tf_record_creation_util.mkdir_p(os.path.join(FLAGS.output_dir, 'train'))
+  train_output_path = os.path.join(FLAGS.output_dir, 'train', 'coco.record')
+  tf_record_creation_util.mkdir_p(os.path.join(FLAGS.output_dir, 'val'))
+  val_output_path = os.path.join(FLAGS.output_dir, 'val', 'coco.record')
+  tf_record_creation_util.mkdir_p(os.path.join(FLAGS.output_dir, 'testdev'))
+  testdev_output_path = os.path.join(FLAGS.output_dir, 'testdev', 'coco.record')
+  train_image_output_path = None
+  val_image_output_path = None
+  testdev_image_output_path = None
+  if FLAGS.image_save_period > 0:
+    tf_record_creation_util.mkdir_p(os.path.join(FLAGS.output_dir, 'train', 'images'))
+    train_image_output_path = os.path.join(FLAGS.output_dir, 'train', 'images')
+    tf_record_creation_util.mkdir_p(os.path.join(FLAGS.output_dir, 'val', 'images'))
+    val_image_output_path = os.path.join(FLAGS.output_dir, 'val', 'images')
+    tf_record_creation_util.mkdir_p(os.path.join(FLAGS.output_dir, 'testdev', 'images'))
+    testdev_image_output_path = os.path.join(FLAGS.output_dir, 'testdev', 'images')
 
   _create_tf_record_from_coco_annotations(
       FLAGS.train_annotations_file,
       FLAGS.train_image_dir,
       train_output_path,
+      train_image_output_path,
       FLAGS.include_masks,
+      FLAGS.encoded_type,
+      FLAGS.image_save_period,
       num_shards=100)
   _create_tf_record_from_coco_annotations(
       FLAGS.val_annotations_file,
       FLAGS.val_image_dir,
       val_output_path,
+      val_image_output_path,
       FLAGS.include_masks,
+      FLAGS.encoded_type,
+      FLAGS.image_save_period,
       num_shards=10)
   _create_tf_record_from_coco_annotations(
       FLAGS.testdev_annotations_file,
       FLAGS.test_image_dir,
       testdev_output_path,
+      testdev_image_output_path,
       FLAGS.include_masks,
+      FLAGS.encoded_type,
+      FLAGS.image_save_period,
       num_shards=100)
 
 
diff --git a/research/object_detection/dataset_tools/create_kitti_tf_record.py b/research/object_detection/dataset_tools/create_kitti_tf_record.py
index c612db99..ef295111 100644
--- a/research/object_detection/dataset_tools/create_kitti_tf_record.py
+++ b/research/object_detection/dataset_tools/create_kitti_tf_record.py
@@ -41,9 +41,10 @@ import io
 import os
 
 import numpy as np
-import PIL.Image as pil
+import PIL.Image
 import tensorflow as tf
 
+from object_detection.dataset_tools import tf_record_creation_util
 from object_detection.utils import dataset_util
 from object_detection.utils import label_map_util
 from object_detection.utils.np_box_ops import iou
@@ -66,11 +67,15 @@ tf.app.flags.DEFINE_string('label_map_path', 'data/kitti_label_map.pbtxt',
                            'Path to label map proto.')
 tf.app.flags.DEFINE_integer('validation_set_size', '500', 'Number of images to'
                             'be used as a validation set.')
+tf.app.flags.DEFINE_string('encoded_type', 'jpg', 'Image type to encode (jpg or png).')
+tf.app.flags.DEFINE_integer('image_save_period', '0', 'Save a JPEG image every'
+                            '`image_save_period` image file.')
 FLAGS = tf.app.flags.FLAGS
 
 
 def convert_kitti_to_tfrecords(data_dir, output_path, classes_to_use,
-                               label_map_path, validation_set_size):
+                               label_map_path, validation_set_size,
+                               encoded_type, image_save_period):
   """Convert the KITTI detection dataset to TFRecords.
 
   Args:
@@ -94,6 +99,9 @@ def convert_kitti_to_tfrecords(data_dir, output_path, classes_to_use,
   label_map_dict = label_map_util.get_label_map_dict(label_map_path)
   train_count = 0
   val_count = 0
+  encoded_type = encoded_type.lower()
+  if encoded_type == 'jpg':
+    encoded_type = 'jpeg'
 
   annotation_dir = os.path.join(data_dir,
                                 'training',
@@ -104,13 +112,20 @@ def convert_kitti_to_tfrecords(data_dir, output_path, classes_to_use,
                            'training',
                            'image_2')
 
-  train_writer = tf.python_io.TFRecordWriter('%s_train.tfrecord'%
-                                             output_path)
-  val_writer = tf.python_io.TFRecordWriter('%s_val.tfrecord'%
-                                           output_path)
+  tf_record_creation_util.mkdir_p(os.path.join(output_path, 'train'))
+  train_writer = tf.python_io.TFRecordWriter(os.path.join(output_path, 'train', 'kitti.tfrecord'))
+  tf_record_creation_util.mkdir_p(os.path.join(output_path, 'val'))
+  val_writer = tf.python_io.TFRecordWriter(os.path.join(output_path, 'val', 'kitti.tfrecord'))
+  train_image_output_path = None
+  val_image_output_path = None
+  if image_save_period > 0:
+    tf_record_creation_util.mkdir_p(os.path.join(output_path, 'train', 'images'))
+    train_image_output_path = os.path.join(output_path, 'train', 'images')
+    tf_record_creation_util.mkdir_p(os.path.join(output_path, 'val', 'images'))
+    val_image_output_path = os.path.join(output_path, 'val', 'images')
 
   images = sorted(tf.gfile.ListDirectory(image_dir))
-  for img_name in images:
+  for idx, img_name in enumerate(images):
     img_num = int(img_name.split('.')[0])
     is_validation_img = img_num < validation_set_size
     img_anno = read_annotation_file(os.path.join(annotation_dir,
@@ -123,7 +138,13 @@ def convert_kitti_to_tfrecords(data_dir, output_path, classes_to_use,
     # TODO(talremez) filter out targets that are truncated or heavily occluded.
     annotation_for_image = filter_annotations(img_anno, classes_to_use)
 
-    example = prepare_example(image_path, annotation_for_image, label_map_dict)
+    save_image = False if image_save_period == 0 else idx % image_save_period == 0
+    if is_validation_img:
+      image_output_path = val_image_output_path
+    else:
+      image_output_path = train_image_output_path
+    example = prepare_example(image_path, annotation_for_image, label_map_dict,
+                              encoded_type, save_image, image_output_path)
     if is_validation_img:
       val_writer.write(example.SerializeToString())
       val_count += 1
@@ -135,7 +156,8 @@ def convert_kitti_to_tfrecords(data_dir, output_path, classes_to_use,
   val_writer.close()
 
 
-def prepare_example(image_path, annotations, label_map_dict):
+def prepare_example(image_path, annotations, label_map_dict,
+                    encoded_type, save_image, image_output_path):
   """Converts a dictionary with annotations for an image to tf.Example proto.
 
   Args:
@@ -150,10 +172,19 @@ def prepare_example(image_path, annotations, label_map_dict):
   with tf.gfile.GFile(image_path, 'rb') as fid:
     encoded_png = fid.read()
   encoded_png_io = io.BytesIO(encoded_png)
-  image = pil.open(encoded_png_io)
+  image = PIL.Image.open(encoded_png_io)
+  filename = os.path.basename(image_path)
+  if encoded_type == 'png':
+    encoded_image = tf_record_creation_util.convert_to_png(image)
+    if save_image:
+      image.save(os.path.join(image_output_path, filename[:filename.rfind('.')] + '.png'))
+  elif encoded_type == 'jpeg':
+    encoded_image = tf_record_creation_util.convert_to_jpeg(image)
+    if save_image:
+      image.save(os.path.join(image_output_path, filename[:filename.rfind('.')] + '.jpeg'))
   image = np.asarray(image)
 
-  key = hashlib.sha256(encoded_png).hexdigest()
+  key = hashlib.sha256(encoded_image).hexdigest()
 
   width = int(image.shape[1])
   height = int(image.shape[0])
@@ -171,8 +202,8 @@ def prepare_example(image_path, annotations, label_map_dict):
       'image/filename': dataset_util.bytes_feature(image_path.encode('utf8')),
       'image/source_id': dataset_util.bytes_feature(image_path.encode('utf8')),
       'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),
-      'image/encoded': dataset_util.bytes_feature(encoded_png),
-      'image/format': dataset_util.bytes_feature('png'.encode('utf8')),
+      'image/encoded': dataset_util.bytes_feature(encoded_image),
+      'image/format': dataset_util.bytes_feature(encoded_type.encode('utf8')),
       'image/object/bbox/xmin': dataset_util.float_list_feature(xmin_norm),
       'image/object/bbox/xmax': dataset_util.float_list_feature(xmax_norm),
       'image/object/bbox/ymin': dataset_util.float_list_feature(ymin_norm),
@@ -304,7 +335,9 @@ def main(_):
       output_path=FLAGS.output_path,
       classes_to_use=FLAGS.classes_to_use.split(','),
       label_map_path=FLAGS.label_map_path,
-      validation_set_size=FLAGS.validation_set_size)
+      validation_set_size=FLAGS.validation_set_size,
+      encoded_type=FLAGS.encoded_type,
+      image_save_period=FLAGS.image_save_period)
 
 if __name__ == '__main__':
   tf.app.run()
diff --git a/research/object_detection/dataset_tools/tf_record_creation_util.py b/research/object_detection/dataset_tools/tf_record_creation_util.py
index e8da2291..3b3c3732 100644
--- a/research/object_detection/dataset_tools/tf_record_creation_util.py
+++ b/research/object_detection/dataset_tools/tf_record_creation_util.py
@@ -18,9 +18,39 @@ from __future__ import absolute_import
 from __future__ import division
 from __future__ import print_function
 
+import io
+import os
+import sys
+import errno
 import tensorflow as tf
 
 
+def convert_to_jpeg(im):
+  with io.BytesIO() as f:
+    im.save(f, format='JPEG')
+    return f.getvalue()
+
+
+def convert_to_png(im):
+  with io.BytesIO() as f:
+    im.save(f, format='PNG')
+    return f.getvalue()
+
+
+def mkdir_p(path):
+  """Convience function to mimic the "mkdir -p" Unix command."""
+  if (2, 5) <= sys.version_info < (3, 2):
+    try:
+      os.makedirs(path)
+    except OSError as exc:  # Python >2.5
+      if exc.errno == errno.EEXIST and os.path.isdir(path):
+        pass
+      else:
+        raise
+  else:
+    os.makedirs(path, exist_ok=True)  # noqa pylint: disable=E1123
+
+
 def open_sharded_output_tfrecords(exit_stack, base_path, num_shards):
   """Opens all TFRecord shards for writing and adds them to an exit stack.
 
diff --git a/research/object_detection/model_lib.py b/research/object_detection/model_lib.py
index 5ba6f9b6..2239bb8b 100644
--- a/research/object_detection/model_lib.py
+++ b/research/object_detection/model_lib.py
@@ -411,7 +411,7 @@ def create_model_fn(detection_model_fn, configs, hparams, use_tpu=False):
 
       # Eval metrics on a single example.
       eval_metric_ops = eval_util.get_eval_metric_ops_for_evaluators(
-          eval_config, category_index.values(), eval_dict)
+          eval_config, list(category_index.values()), eval_dict)
       for loss_key, loss_tensor in iter(losses_dict.items()):
         eval_metric_ops[loss_key] = tf.metrics.mean(loss_tensor)
       for var in optimizer_summary_vars:
diff --git a/research/object_detection/models/ssd_resnet_v1_feature_extractor.py b/research/object_detection/models/ssd_resnet_v1_feature_extractor.py
new file mode 100644
index 00000000..8d26917c
--- /dev/null
+++ b/research/object_detection/models/ssd_resnet_v1_feature_extractor.py
@@ -0,0 +1,373 @@
+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.
+#
+# Licensed under the Apache License, Version 2.0 (the "License");
+# you may not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+# ==============================================================================
+
+"""SSDFeatureExtractor for ResnetV1 features."""
+
+import tensorflow as tf
+
+from object_detection.meta_architectures import ssd_meta_arch
+from object_detection.models import feature_map_generators
+from object_detection.utils import context_manager
+from object_detection.utils import ops
+from object_detection.utils import shape_utils
+from nets import resnet_v1
+from nets.mobilenet import mobilenet
+from nets.mobilenet import mobilenet_v2
+
+slim = tf.contrib.slim
+
+
+class _SSDResnetV1FeatureExtractor(ssd_meta_arch.SSDFeatureExtractor):
+  """SSD Feature Extractor using ResnetV1 features."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               resnet_base_fn,
+               resnet_scope_name,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """ResnetV1 Feature Extractor for SSD Models.
+
+    Resnet v1.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the
+        base feature extractor.
+      resnet_base_fn: base resnet network to use.
+      resnet_scope_name: scope name under which to construct resnet
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+    """
+    super(_SSDResnetV1FeatureExtractor, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
+        override_base_feature_extractor_hyperparams)
+    if self._depth_multiplier != 1.0:
+      raise ValueError('Only depth 1.0 is supported, found: {}'.
+                       format(self._depth_multiplier))
+    if self._use_explicit_padding is True:
+      raise ValueError('Explicit padding is not a valid option.')
+    self._resnet_base_fn = resnet_base_fn
+    self._resnet_scope_name = resnet_scope_name
+
+  def preprocess(self, resized_inputs):
+    """SSD preprocessing.
+
+    Maps pixel values to the range [-1, 1].
+
+    Args:
+      resized_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+    """
+    return (2.0 / 255.0) * resized_inputs - 1.0
+
+  def _filter_features(self, image_features):
+    # TODO(rathodv): Change resnet endpoint to strip scope prefixes instead
+    # of munging the scope here.
+    filtered_image_features = dict({})
+    for key, feature in image_features.items():
+      feature_name = key.split('/')[-1]
+      if feature_name in ['block1', 'block2', 'block3', 'block4', 'block5']:
+        filtered_image_features[feature_name] = feature
+    return filtered_image_features
+
+  def extract_features(self, preprocessed_inputs):
+    """Extract features from preprocessed inputs.
+
+    Args:
+      preprocessed_inputs: a [batch, height, width, channels] float tensor
+        representing a batch of images.
+
+    Returns:
+      feature_maps: a list of tensors where the ith tensor has shape
+        [batch, height_i, width_i, depth_i]
+    """
+    if self._depth_multiplier != 1.0:
+      raise ValueError('Depth multiplier not supported.')
+
+    preprocessed_inputs = shape_utils.check_min_image_dim(
+        33, preprocessed_inputs)
+
+    feature_map_layout_4 = {
+        'from_layer': ['block3', 'block4', '', '', '', '', ''],
+        'layer_depth': [-1, -1, 512, 256, 256, 128],
+        'use_depthwise': self._use_depthwise,
+        'use_explicit_padding': self._use_explicit_padding,
+    }
+    feature_map_layout_5 = {
+        'from_layer': ['block4', 'block5', '', '', '', ''],
+        'layer_depth': [-1, -1, 512, 256, 256, 128],
+        'use_depthwise': self._use_depthwise,
+        'use_explicit_padding': self._use_explicit_padding,
+    }
+
+    scope = self._resnet_scope_name
+    with slim.arg_scope(resnet_v1.resnet_arg_scope()):
+      with (slim.arg_scope(self._conv_hyperparams_fn())
+            if self._override_base_feature_extractor_hyperparams else
+            context_manager.IdentityContextManager()):
+        _, image_features = self._resnet_base_fn(
+            inputs=ops.pad_to_multiple(preprocessed_inputs,
+                                       self._pad_to_multiple),
+            num_classes=None,
+            is_training=None,
+            global_pool=False,
+            output_stride=None,
+            store_non_strided_activations=True,
+            reuse=self._reuse_weights,
+            scope=scope)
+        image_features = self._filter_features(image_features)
+      feature_map_layout = feature_map_layout_4 if len(image_features) == 4 else feature_map_layout_5
+      with slim.arg_scope(self._conv_hyperparams_fn()):
+        feature_maps = feature_map_generators.multi_resolution_feature_maps(
+            feature_map_layout=feature_map_layout,
+            depth_multiplier=self._depth_multiplier,
+            min_depth=self._min_depth,
+            insert_1x1_conv=True,
+            image_features=image_features)
+
+    return feature_maps.values()
+
+
+class SSDResnet18V1FeatureExtractor(_SSDResnetV1FeatureExtractor):
+  """SSD Resnet18 V1 feature extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """Resnet18 V1 feature extractor for SSD Models.
+
+    Resnet v1.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the
+        base feature extractor.
+      resnet_base_fn: base resnet network to use.
+      resnet_scope_name: scope name under which to construct resnet
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+    """
+    super(SSDResnet18V1FeatureExtractor, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        resnet_base_fn=resnet_v1.resnet_v1_18,
+        resnet_scope_name='resnet18_nvidia',
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
+        override_base_feature_extractor_hyperparams)
+
+
+class SSDResnet50V1FeatureExtractor(_SSDResnetV1FeatureExtractor):
+  """SSD Resnet50 V1 feature extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """Resnet50 V1 feature extractor for SSD Models.
+
+    Resnet v1.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the
+        base feature extractor.
+      resnet_base_fn: base resnet network to use.
+      resnet_scope_name: scope name under which to construct resnet
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+    """
+    super(SSDResnet50V1FeatureExtractor, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        resnet_base_fn=resnet_v1.resnet_v1_50,
+        resnet_scope_name='resnet_v1_50',
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
+        override_base_feature_extractor_hyperparams)
+
+
+class SSDResnet101V1FeatureExtractor(_SSDResnetV1FeatureExtractor):
+  """SSD Resnet101 V1 feature extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """Resnet101 V1 feature extractor for SSD Models.
+
+    Resnet v1.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the
+        base feature extractor.
+      resnet_base_fn: base resnet network to use.
+      resnet_scope_name: scope name under which to construct resnet
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+    """
+    super(SSDResnet101V1FeatureExtractor, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        resnet_base_fn=resnet_v1.resnet_v1_101,
+        resnet_scope_name='resnet_v1_101',
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
+        override_base_feature_extractor_hyperparams)
+
+
+class SSDResnet152V1FeatureExtractor(_SSDResnetV1FeatureExtractor):
+  """SSD Resnet152 V1 feature extractor."""
+
+  def __init__(self,
+               is_training,
+               depth_multiplier,
+               min_depth,
+               pad_to_multiple,
+               conv_hyperparams_fn,
+               reuse_weights=None,
+               use_explicit_padding=False,
+               use_depthwise=False,
+               override_base_feature_extractor_hyperparams=False):
+    """Resnet152 V1 feature extractor for SSD Models.
+
+    Resnet v1.
+
+    Args:
+      is_training: whether the network is in training mode.
+      depth_multiplier: float depth multiplier for feature extractor.
+      min_depth: minimum feature extractor depth.
+      pad_to_multiple: the nearest multiple to zero pad the input height and
+        width dimensions to.
+      conv_hyperparams_fn: A function to construct tf slim arg_scope for conv2d
+        and separable_conv2d ops in the layers that are added on top of the
+        base feature extractor.
+      resnet_base_fn: base resnet network to use.
+      resnet_scope_name: scope name under which to construct resnet
+      reuse_weights: Whether to reuse variables. Default is None.
+      use_explicit_padding: Whether to use explicit padding when extracting
+        features. Default is False.
+      use_depthwise: Whether to use depthwise convolutions. Default is False.
+      override_base_feature_extractor_hyperparams: Whether to override
+        hyperparameters of the base feature extractor with the one from
+        `conv_hyperparams_fn`.
+    """
+    super(SSDResnet152V1FeatureExtractor, self).__init__(
+        is_training=is_training,
+        depth_multiplier=depth_multiplier,
+        min_depth=min_depth,
+        pad_to_multiple=pad_to_multiple,
+        conv_hyperparams_fn=conv_hyperparams_fn,
+        resnet_base_fn=resnet_v1.resnet_v1_152,
+        resnet_scope_name='resnet_v1_152',
+        reuse_weights=reuse_weights,
+        use_explicit_padding=use_explicit_padding,
+        use_depthwise=use_depthwise,
+        override_base_feature_extractor_hyperparams=
+        override_base_feature_extractor_hyperparams)
diff --git a/research/object_detection/predictors/heads/box_head.py b/research/object_detection/predictors/heads/box_head.py
index 3adfecb7..49d64145 100644
--- a/research/object_detection/predictors/heads/box_head.py
+++ b/research/object_detection/predictors/heads/box_head.py
@@ -20,6 +20,7 @@ All the box prediction heads have a predict function that receives the
 `features` as the first argument and returns `box_encodings`.
 """
 import functools
+import numpy as np
 import tensorflow as tf
 
 from object_detection.predictors.heads import head
@@ -180,8 +181,10 @@ class ConvolutionalBoxHead(head.Head):
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
       batch_size = tf.shape(features)[0]
+    be_shape = box_encodings.get_shape().as_list()
     box_encodings = tf.reshape(box_encodings,
-                               [batch_size, -1, 1, self._box_code_size])
+                               [batch_size, np.prod(be_shape[1:])//self._box_code_size, 1, self._box_code_size]
+                               )
     return box_encodings
 
 
diff --git a/research/object_detection/predictors/heads/class_head.py b/research/object_detection/predictors/heads/class_head.py
index ed28cbc4..5249b6f8 100644
--- a/research/object_detection/predictors/heads/class_head.py
+++ b/research/object_detection/predictors/heads/class_head.py
@@ -20,6 +20,7 @@ All the class prediction heads have a predict function that receives the
 `features` as the first argument and returns class predictions with background.
 """
 import functools
+import numpy as np
 import tensorflow as tf
 
 from object_detection.predictors.heads import head
@@ -193,8 +194,10 @@ class ConvolutionalClassHead(head.Head):
     batch_size = features.get_shape().as_list()[0]
     if batch_size is None:
       batch_size = tf.shape(features)[0]
-    class_predictions_with_background = tf.reshape(
-        class_predictions_with_background, [batch_size, -1, num_class_slots])
+    cpwb_shape = class_predictions_with_background.get_shape().as_list()
+    class_predictions_with_background = tf.reshape(class_predictions_with_background,
+                                                   [batch_size, np.prod(cpwb_shape[1:])//num_class_slots, num_class_slots]
+                                                  )
     return class_predictions_with_background
 
 
diff --git a/research/slim/nets/resnet_v1.py b/research/slim/nets/resnet_v1.py
index 95e1a11c..6683067f 100644
--- a/research/slim/nets/resnet_v1.py
+++ b/research/slim/nets/resnet_v1.py
@@ -115,7 +115,8 @@ def bottleneck(inputs,
     else:
       shortcut = slim.conv2d(
           inputs,
-          depth, [1, 1],
+          depth,
+          [1, 1],
           stride=stride,
           activation_fn=tf.nn.relu6 if use_bounded_activations else None,
           scope='shortcut')
@@ -139,6 +140,70 @@ def bottleneck(inputs,
                                             output)
 
 
+@slim.add_arg_scope
+def bottleneck_resnet18(inputs,
+                        depth,
+                        depth_bottleneck,
+                        stride,
+                        rate=1,
+                        first_block=False,
+                        outputs_collections=None,
+                        scope=None,
+                        use_bounded_activations=False):
+  """Bottleneck residual unit variant with BN after convolutions.
+
+  This is the original residual unit proposed in [1]. See Fig. 1(a) of [2] for
+  its definition. Note that we use here the bottleneck variant which has an
+  extra bottleneck layer.
+
+  When putting together two consecutive ResNet blocks that use this unit, one
+  should use stride = 2 in the last unit of the first block.
+
+  Args:
+    inputs: A tensor of size [batch, height, width, channels].
+    depth: The depth of the ResNet unit output.
+    depth_bottleneck: The depth of the bottleneck layers.
+    stride: The ResNet unit's stride. Determines the amount of downsampling of
+      the units output compared to its input.
+    rate: An integer, rate for atrous convolution.
+    outputs_collections: Collection to add the ResNet unit output.
+    scope: Optional variable_scope.
+    use_bounded_activations: Whether or not to use bounded activations. Bounded
+      activations better lend themselves to quantized inference.
+
+  Returns:
+    The ResNet unit's output.
+  """
+  with tf.variable_scope(scope, 'bottleneck_resnet18_nvidia', [inputs]) as sc:
+    depth_in = slim.utils.last_dimension(inputs.get_shape(), min_rank=4)
+    if depth == depth_in and first_block == False:
+      shortcut = resnet_utils.subsample(inputs, stride, 'shortcut')
+    else:
+      shortcut = slim.conv2d(
+          inputs,
+          depth,
+          [1, 1],
+          stride=stride,
+          activation_fn=tf.nn.relu6 if use_bounded_activations else None,
+          scope='shortcut')
+
+    residual = slim.conv2d(inputs, depth_bottleneck, 3, stride=1,
+                           scope='conv1')
+    residual = resnet_utils.conv2d_same(residual, depth_bottleneck, 3, stride,
+                                        rate=rate, scope='conv2')
+
+    if use_bounded_activations:
+      # Use clip_by_value to simulate bandpass activation.
+      residual = tf.clip_by_value(residual, -6.0, 6.0)
+      output = tf.nn.relu6(shortcut + residual)
+    else:
+      output = tf.nn.relu(shortcut + residual)
+
+    return slim.utils.collect_named_outputs(outputs_collections,
+                                            sc.name,
+                                            output)
+
+
 def resnet_v1(inputs,
               blocks,
               num_classes=None,
@@ -221,6 +286,7 @@ def resnet_v1(inputs,
   with tf.variable_scope(scope, 'resnet_v1', [inputs], reuse=reuse) as sc:
     end_points_collection = sc.original_name_scope + '_end_points'
     with slim.arg_scope([slim.conv2d, bottleneck,
+                         bottleneck_resnet18,
                          resnet_utils.stack_blocks_dense],
                         outputs_collections=end_points_collection):
       with (slim.arg_scope([slim.batch_norm], is_training=is_training)
@@ -279,6 +345,67 @@ def resnet_v1_block(scope, base_depth, num_units, stride):
   }])
 
 
+def resnet18_v1_block(scope, base_depth, num_units, stride, first_block=False):
+  """Helper function for creating a resnet-18 bottleneck block.
+
+  Args:
+    scope: The scope of the block.
+    base_depth: The depth of the bottleneck layer for each unit.
+    num_units: The number of units in the block.
+    stride: The stride of the block, implemented as a stride in the last unit.
+      All other units have stride=1.
+
+  Returns:
+    A resnet18 bottleneck block.
+  """
+  # Expect num_units to be 1 or 2
+  if not(num_units in [1,2]):
+      raise ValueError('Expect num_units to be 1 or 2.')
+
+  if num_units == 2:
+      base_depth_front = base_depth[0]
+      base_depth_back = base_depth[1]
+  else:
+      base_depth_front = None
+      base_depth_back = base_depth[0]
+
+  return resnet_utils.Block(scope, bottleneck_resnet18, [{
+      'depth': base_depth_front,
+      'depth_bottleneck': base_depth_front,
+      'stride': 1
+  }] * (num_units - 1) + [{
+      'depth': base_depth_back,
+      'depth_bottleneck': base_depth_back,
+      'stride': stride,
+      'first_block': first_block
+  }])
+
+
+def resnet_v1_18(inputs,
+                 num_classes=None,
+                 is_training=True,
+                 global_pool=True,
+                 output_stride=None,
+                 spatial_squeeze=True,
+                 store_non_strided_activations=False,
+                 reuse=None,
+                 scope='resnet_v1_18'):
+  """ResNet-18 model of [1]. See resnet_v1() for arg and return description."""
+  blocks = [
+      resnet18_v1_block('block1', base_depth=[64,64], num_units=1, stride=1, first_block=True),
+      resnet18_v1_block('block2', base_depth=[64,128], num_units=2, stride=2),
+      resnet18_v1_block('block3', base_depth=[128,256], num_units=2, stride=2),
+      resnet18_v1_block('block4', base_depth=[256,512], num_units=2, stride=2),
+      resnet18_v1_block('block5', base_depth=[512], num_units=1, stride=1),
+  ]
+  return resnet_v1(inputs, blocks, num_classes, is_training,
+                   global_pool=global_pool, output_stride=output_stride,
+                   include_root_block=True, spatial_squeeze=spatial_squeeze,
+                   store_non_strided_activations=store_non_strided_activations,
+                   reuse=reuse, scope=scope)
+resnet_v1_18.default_image_size = resnet_v1.default_image_size
+
+
 def resnet_v1_50(inputs,
                  num_classes=None,
                  is_training=True,
